# Malware-Analysis

using Drebin dataset to distinguish between malwares and not malwares

## Libraries

- python 3.7.1 (x64)
- pip 18.1
- NumPy 1.15.4
- SciPy 1.1.0
- Sklearn 0.20.0
- Pandas 0.23.4

## How it works

1. Open `malware_analysis.py` and run it

   1. we first use `x, y = read_dataset.read_data()`
      - which uses pandas to import the csv file of drebin dataset (download from [here](https://drive.google.com/file/d/0Bxxqx_AAp2u2enI0UzBqSEZQRHc/edit))
      - prints out the size of the dataset and classify if the file is malware or not based on if the file name is found in csv file or not, and prints number of malwares found and number of safe files
      - Extract features found in each file and if malware labels it 1, 0 otherwise
        - `sample = features_extraction.extract_features(file_content)`
          - uses method implemented in `features_extraction.py`
          - define a features set dictionary including all the features that can be used to detect a malware
          - the method `extract_features(file_content)` creates an empty dictionary to have the number of features occurrences in the input -which is a text file-
          - Whenever a feature extracted from file content, and this feature is found in the features set dictionary it increments its corresponding in the occurrences dictionary we created
          - in the end we copy the values of the dictionary to an array and return it
      - convert the arrays of feature vectors and labels to numpy arrays and return them in variables x & y
   2. And then we select the features we want for demonstration

      - basically all do the same thing in terms of coding them -not in terms of what happens under the hood-, we fit all the data in to get to know which features have the most impact on the models and thus we use them
        - `features_selection.select_features_k_best(x, y)`
          - scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.
          - uses the chi squared (chi^2) statistical test for non-negative features to select 4 of the best features from the dataset
        - `features_selection.select_features_recursive_feature_elimination(x, y)`
          - It works by recursively removing attributes and building a model on those attributes that remain.
          - It works by recursively removing attributes and building a model on those attributes that remain.
        - `features_selection.select_features_extra_trees(x, y)`
        - `features_selection.select_features_random_forest(x, y)`
          - both of random forest and extra trees, we can say they are approximately the same in terms of how they work
            Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.

---

_Note_ that, the code balances the dataset based on the malware files size, you can change that by removing the part the states 'remove this for using unbalanced dataset' in read_dataset.py file

---

<b>Output:</b>

<h2> Features Selection using <b>unbalanced data</b></h2>
<img src="Extras/K BEST classifier features importance output.bmp" alt="K BEST classifier features importance output" title="K BEST classifier features importance output"/>

<img src="Extras/RFE classifier features importance output.bmp" alt="RFE classifier features importance output" title="RFE classifier features importance output"/>

<img src="Extras/extra trees classifier features importance graph 1.bmp" alt="extra trees classifier features importance graph 1" title="extra trees classifier classifier features importance graph"/>

<img src="Extras/random forrest classifier features importance graph 1.bmp" alt="random forrest classifier features importance graph 1" title="random forrest classifier features importance graph"/>

<h2> Features Selection using <b>balanced data</b></h2>
<img src="Extras/K BEST classifier features importance output balanced dataset.png" alt="K BEST classifier features importance output balanced dataset" title="K BEST classifier features importance output balanced dataset"/>

<img src="Extras/RFE classifier features importance output balanced dataset.png" alt="RFE classifier features importance output balanced dataset" title="RFE classifier features importance output balanced dataset"/>

<img src="Extras/Figure_1 balanced dataset.png" alt="extra trees classifier features importance graph 1 balanced dataset" title="extra trees classifier classifier features importance graph balanced dataset"/>

<img src="Extras/Figure_2 balanced dataset.png" alt="random forrest classifier features importance graph balanced dataset" title="random forrest classifier features importance graph balanced dataset"/>

---

3.  Using `train_test_split()` function from `sklearn.model_selection` to split our data into 2 categories testing (20%) and training data (80%), and print out their shapes for representation purposes
4.  Use `train.py` which includes all the training models, as well as their performance metrices
    1. fit the model using the training data
       - SVM
       - Extra Trees
       - Random forest
       - Recursive features elimination
       - Naive Bayes
    2. Predict the x_test
    3. Build confusion matrix, and use the evaluation metrices
       - Accuracy
       - Precision
       - Recall
       - F1 score

---

- But why we do features selection?

  - Three benefits of performing feature selection before modeling your data are:

  1. Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.
  2. Improves Accuracy: Less misleading data means modeling accuracy improves.
  3. Reduces Training Time: Less data means that algorithms train faster.

---

- Difference between Extra trees classifier and Random forest classifier?
  - First thing first, extra trees is named Extremely Randomized Tress
  - Secondly, Random Forest and Extremely Randomized Trees differ in the sense that the splits of the trees in the Random Forest are deterministic whereas they are random in the case of an Extremely Randomized Trees (to be more accurate, the next split is the best split among random uniform splits in the selected variables for the current tree).
  - Extra trees seem to keep a higher performance in presence of noisy features.
  - When all the variables are relevant, both methods seem to achieve the same performance, Extra trees seem three times faster than the random forest (at least, in scikit learn implementation)

---

### Performance Metrices

- Accuracy

  - how many instances were classified correctly
  - (TP + TN) / (TP + FN + TN + FP)

- Precision:

  - It talks about how precise/accurate your model is out of those predicted positive, how many of them are actual positive.
  - It is a good measure to determine, when the costs of False Positive is high.
  - True positives per Total predicted positives (True Positive/ True Positive + False Positive)

- Recall

  - It actually calculates how many of the Actual Positives our model capture through labeling it as Positive (True Positive).
  - It shall be the model metric we use to select our best model when there is a high cost associated with False Negative.
  - True positives / True Positive + False Negative

- F1 Score
  - It might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives)
  - 2 x ((Precision x Recall) / (Precision + Recall))

---

<b> Performance metrices</b>

<h2> Performance metrices of algorithms used to train model using <b>UNBALANCED dataset</b></h2>
<img src="Extras/Performance metrices of algorithms used to train model.png" alt="Performance metrices of algorithms used to train model.png" title="Performance metrices of algorithms used to train model"/>

<h2> Performance metrices of algorithms used to train model using <b>BALANCED dataset</b></h2>
<img src="Extras/Performance metrices of algorithms used to train model balanced data.png" alt="Performance metrices of algorithms used to train model balanced data.png" title="Performance metrices of algorithms used to train model  balanced data"/>

---
